# 高性能计算专家学习路线规划

## 引言：为什么我们必须向下扎根？

在当今时代，大模型的能力令人震撼。然而，驱动这些模型背后的，不是上层算法的奇思妙想，而是底层系统无休止的性能优化和资源管理。我是一个渴望成为"高性能计算专家"的新手，目标是精通 AI 框架层和算子层，能让 Transformer 以最快、最省资源的方式运行。本号将从今天起，记录我的全部学习历程。我的路线图，不是随便挑选的教程，而是汇集了 MIT、Stanford、CMU 等全球顶尖名校的十五门天花板级课程。这条路注定艰难，但它将是通往 AI Infra God 的最快、最科学的路径。

## 专家的十五门必修课

我的学习路线图覆盖了从数学基石到 LLM 系统的全部知识栈：

**数学与算法基石**：MIT 18.06（线性代数）、MIT 18.335J（数值方法）、CS61B（数据结构）。

**工程与系统之魂**：CS50P（Python 编程）、CS106L（C++）、CS61C（计算机体系结构）、MIT 6.S081（操作系统）。

**算法与模型核心**：CS231N（深度学习与计算机视觉）、CS224N（自然语言处理）、CMU 11-777（高级深度学习）。

**加速与系统集成**：CS143（编译原理）、CS149（并行计算）、CS144（计算机网络）、CS336（语言模型系统）。

## 🛣️ 路线规划：科学且高效的进阶路径

我将遵循"先基石、后系统、再算法、终加速"的原则进行学习。每一步都以前一步的知识为强依赖，确保不走弯路，不留知识盲区。

### 阶段一：奠定基石 (The Foundations)

**核心目标**：建立编程思维、算法逻辑和张量运算的数学基础。

- **CS50P (Python 编程)**：掌握工程思维，这是所有 AI 框架的上层接口语言。
- **MIT 18.06 (线性代数)**：奠定矩阵运算的数学基石，理解张量（Tensor）的本质。
- **CS61B (数据结构)**：训练算法设计和复杂度分析能力，这对 KV Cache 优化至关重要。

### 阶段二：系统之魂与硬核工程 (System Roots & C++)

**核心目标**：理解代码如何在硬件上运行，掌握高性能语言和数值计算的真相。

- **CS106L (C++)**：掌握现代 C++ 写法，它是高性能算子和框架后端的母语。
- **CS61C (计算机体系结构)**：掌握 Cache、SIMD 等，理解内存访问模式，这是 Kernel 优化的核心心法。
- **MIT 18.335J (数值方法)**：必须掌握浮点数运算和数值稳定性，理解 Quantization 的精度损失。
- **MIT 6.S081 (操作系统)**：理解内存分配、进程和调度，这是框架进行显存管理的基础。

### 阶段三：算法、模型与加速引擎 (Algorithm & Acceleration)

**核心目标**：理解你要加速的模型架构和加速机制（编译器）。

- **CS231N (深度学习) & CS224N (NLP)**：掌握 Transformer 架构的计算细节，特别是自注意力机制。
- **CMU 11-777 (高级深度学习)**：掌握最新的模型架构和计算挑战，保持对前沿技术的敏感度。
- **CS143 (编译原理)**：理解 IR（中间表示）和图优化。这是 TensorRT-LLM、PyTorch Inductor 等 AI 编译器的底层逻辑。

### 阶段四：专业加速与系统集成 (Synthesis & Specialization)

**核心目标**：将所有知识整合，构建和优化大规模 LLM 推理系统。

- **CS149 (并行计算)**：深入理解 GPU 的编程模型，为编写 CUDA 或 Triton 算子做准备。
- **CS144 (计算机网络)**：掌握分布式训练与推理、高并发服务部署的网络基础。
- **CS336 (语言模型系统)**：终极课程。将所有知识应用于 LLM，学习 FlashAttention、KV Cache 优化和 Serving 架构等最前沿技术。

## 🔍 本号的定位与价值

从下一篇文章开始，我将以"课程代号 + 学习内容"的格式，详细输出我的学习笔记和 Project 实践心得。这不是简单的翻译：我将结合 AI Infra 的视角，深度解析这些课程中的知识点如何应用到 PyTorch、Triton、vLLM 等实际工具中。

**硬核实战**：我的学习笔记将重点关注课程中的实验（如 Malloc Lab、Cache Lab），并尝试用 AI Infra 的方式重新实现和优化。

如果你也渴望成为一名真正向下扎根、能够深入框架和算子层面的技术专家，请关注我，让我们一起踏上这条艰难但充满回报的高性能计算之路！